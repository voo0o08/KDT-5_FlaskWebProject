{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BartConfig, BartTokenizer, BartModel\n",
    "\n",
    "# config = BartConfig.from_pretrained(\"facebook/bart-large\")\n",
    "# tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "# model = BartModel.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dylanmengzhou/kobart-trans-en-ko-v2\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"dylanmengzhou/kobart-trans-en-ko-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Your input_length: 31 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "c:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Your input_length: 91 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': '우리 주 예수 그리스도의 은혜가 너희와 함께 하십시요. 아멘. 주'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 90 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': '그러므로 하루 아침에 역병이 몰려와 죽고 애통함과 기근이 오고 불로'}]\n",
      "[{'translation_text': '그리고 그녀와 함께 살면서 처절한 시련을 겪고 맛있게 살았던 땅의'}]\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"translation\", model=\"circulus/kobart-trans-ko-en-v2\")\n",
    "# pipe(\"오늘 점심으로 스테이크를 먹었습니다.\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"translation\", model=\"dylanmengzhou/kobart-trans-en-ko-v2\")\n",
    "print(pipe(\"The grace of our Lord Jesus Christ be with you all. Amen.\"))\n",
    "print(pipe(\"Therefore shall her plagues come in one day, death, and mourning, and famine; and she shall be utterly burned with fire: for strong is the Lord God who judgeth her.\"))\n",
    "print(pipe(\"And the kings of the earth, who have committed fornication and lived deliciously with her, shall bewail her, and lament for her, when they shall see the smoke of her burning,\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 테이블의 데이터수는 총 31102개 입니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pymysql\n",
    "\n",
    "# 데이터 베이스 연결하기\n",
    "conn = pymysql.connect(host='1.251.203.204',\n",
    "                       user='root',\n",
    "                       password='kdt5',\n",
    "                       db='Team4',\n",
    "                       charset='utf8',\n",
    "                       port=33065)\n",
    "\n",
    "curs = conn.cursor()\n",
    "\n",
    "# 검색 명령어 사용 \n",
    "sql = \"SELECT eng.text as eng_text, kor.text as kor_text FROM language_en eng join language_ko kor on eng.id = kor.id;\"\n",
    "curs.execute(sql)\n",
    "result = curs.fetchall()\n",
    "print(\"현재 테이블의 데이터수는 총 {}개 입니다.\".format(len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdp\\AppData\\Local\\Temp\\ipykernel_24900\\1190723113.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql, conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng_text</th>\n",
       "      <th>kor_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the beginning God created the heaven and th...</td>\n",
       "      <td>태초에 하나님이 천지를 창조하시니라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And the earth was without form, and void; and ...</td>\n",
       "      <td>땅이 혼돈하고 공허하며 흑암이 깊음 위에 있고 하나님의 신은 수면에 운행하시니라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>하나님이 가라사대 빛이 있으라 하시매 빛이 있었고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And God saw the light, that it was good: and G...</td>\n",
       "      <td>그 빛이 하나님의 보시기에 좋았더라 하나님이 빛과 어두움을 나누사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And God called the light Day, and the darkness...</td>\n",
       "      <td>빛을 낮이라 칭하시고 어두움을 밤이라 칭하시니라 저녁이 되며 아침이 되니 이는 첫째...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            eng_text  \\\n",
       "0  In the beginning God created the heaven and th...   \n",
       "1  And the earth was without form, and void; and ...   \n",
       "2  And God said, Let there be light: and there wa...   \n",
       "3  And God saw the light, that it was good: and G...   \n",
       "4  And God called the light Day, and the darkness...   \n",
       "\n",
       "                                            kor_text  \n",
       "0                                태초에 하나님이 천지를 창조하시니라  \n",
       "1       땅이 혼돈하고 공허하며 흑암이 깊음 위에 있고 하나님의 신은 수면에 운행하시니라  \n",
       "2                        하나님이 가라사대 빛이 있으라 하시매 빛이 있었고  \n",
       "3               그 빛이 하나님의 보시기에 좋았더라 하나님이 빛과 어두움을 나누사  \n",
       "4  빛을 낮이라 칭하시고 어두움을 밤이라 칭하시니라 저녁이 되며 아침이 되니 이는 첫째...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_sql(sql, conn)\n",
    "\n",
    "# 데이터베이스 연결 종료\n",
    "conn.close()\n",
    "\n",
    "# 쿼리 -> dataframe결과 출력\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_iterator(df):\n",
    "    for i, row in df.iterrows():\n",
    "        yield row[\"eng_text\"], row[\"kor_text\"] \n",
    "\n",
    "split_idx = int(len(df) * 0.8)  # 전체 데이터 중 80%를 훈련 데이터셋으로 사용\n",
    "train_df = df.iloc[:] # 걍 다 쓰세욤\n",
    "valid_df = df.iloc[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {\"id\":[],\n",
    "              \"translation\":[]}\n",
    "cnt = 0\n",
    "for value in train_df.values:\n",
    "    \n",
    "    train_dict[\"id\"].append(cnt)\n",
    "    \n",
    "    tempdict = {}\n",
    "    tempdict[\"eng\"] = value[0]\n",
    "    tempdict[\"kor\"] = value[1]\n",
    "    train_dict[\"translation\"].append(tempdict)\n",
    "    # print(value)\n",
    "    cnt += 1\n",
    "    # break\n",
    "\n",
    "train_df = pd.DataFrame(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict = {\"id\":[],\n",
    "              \"translation\":[]}\n",
    "cnt = 0\n",
    "for value in valid_df.values:\n",
    "    \n",
    "    val_dict[\"id\"].append(cnt)\n",
    "    \n",
    "    tempdict = {}\n",
    "    tempdict[\"eng\"] = value[0]\n",
    "    tempdict[\"kor\"] = value[1]\n",
    "    val_dict[\"translation\"].append(tempdict)\n",
    "    # print(value)\n",
    "    cnt += 1\n",
    "    # break\n",
    "\n",
    "valid_df = pd.DataFrame(val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 31102\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 6221\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "split_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"validation\": Dataset.from_pandas(valid_df)  # 테스트 데이터도 동일한 데이터프레임을 사용하므로 train과 동일하게 처리\n",
    "})\n",
    "\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eng': 'And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters.',\n",
       " 'kor': '땅이 혼돈하고 공허하며 흑암이 깊음 위에 있고 하나님의 신은 수면에 운행하시니라'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][1][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"dylanmengzhou/kobart-trans-en-ko-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [14603, 309, 299, 21235, 20041, 15562, 16160, 20676, 19542, 20676, 304, 16160, 17884, 315, 21579, 29709, 24536, 20290, 310, 18509, 258, 24536, 18785, 15562, 306, 309, 22873, 20676, 19542, 1700, 316, 311, 14889, 21235, 17254, 26605, 19524, 21235, 18785, 300, 29686, 245, 14603, 309, 299, 21235, 14470, 311, 21738, 16805, 19524, 15464, 20223, 19281, 310, 317, 17752, 1700, 316, 311, 14889, 21235, 17254, 26605, 19524, 21235, 20676, 16881, 27621, 245], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [15495, 12034, 14929, 9869, 14058, 14061, 13644, 14363, 16816, 11711, 12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032, 18466, 19808, 18940, 26418]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"eng\"]\n",
    "ko_sentence = split_datasets[\"train\"][1][\"translation\"][\"kor\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target=ko_sentence)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁땅', '이', '▁혼', '돈', '하고', '▁공', '허', '하며', '▁흑', '암', '이', '▁깊', '음', '▁위에', '▁있고', '▁하나님의', '▁신', '은', '▁수', '면에', '▁운행', '하시', '니라']\n",
      "['▁땅', '이', '▁혼', '돈', '하고', '▁공', '허', '하며', '▁흑', '암', '이', '▁깊', '음', '▁위에', '▁있고', '▁하나님의', '▁신', '은', '▁수', '면에', '▁운행', '하시', '니라']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(ko_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"])) \n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128 \n",
    "\n",
    "def preprocessing_function(examples):\n",
    "    inputs = [ex[\"eng\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"kor\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/31102 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 31102/31102 [00:02<00:00, 11340.37 examples/s]\n",
      "Map: 100%|██████████| 6221/6221 [00:00<00:00, 11416.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = split_datasets.map(\n",
    "\tpreprocessing_function, \n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'decoder_input_ids'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15495, 12034, 14929,  9869, 14058, 14061, 13644, 14363, 16816, 11711,\n",
       "         12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032, 18466,\n",
       "         19808, 18940, 26418],\n",
       "        [14392, 16613, 22279, 11207,  9776, 27888, 17698, 10213, 20853, 10518,\n",
       "         27888, 22658,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 15495, 12034, 14929,  9869, 14058, 14061, 13644, 14363, 16816,\n",
       "         11711, 12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032,\n",
       "         18466, 19808, 18940],\n",
       "        [    1, 14392, 16613, 22279, 11207,  9776, 27888, 17698, 10213, 20853,\n",
       "         10518, 27888, 22658,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15495, 12034, 14929, 9869, 14058, 14061, 13644, 14363, 16816, 11711, 12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032, 18466, 19808, 18940, 26418]\n",
      "[14392, 16613, 22279, 11207, 9776, 27888, 17698, 10213, 20853, 10518, 27888, 22658]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 3):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdp\\AppData\\Local\\Temp\\ipykernel_24900\\200089639.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n",
      "c:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [0, 0, 0, 0],\n",
       " 'totals': [11, 10, 9, 8],\n",
       " 'precisions': [0.0, 0.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 11,\n",
       " 'ref_len': 4}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"In the beginning God created the heaven and the earth.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "    \"태초에 하나님이 천지를 창조하시니라\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [0, 0, 0, 0],\n",
       " 'totals': [29, 28, 27, 26],\n",
       " 'precisions': [0.0, 0.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 29,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"And God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament: and it was so.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "    \"하나님이 궁창을 만드사 궁창 아래의 물과 궁창 위의 물로 나뉘게 하시매 그대로 되니라\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    # 모델이 예측 logits 이상을 반환하는 경우\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # 라벨의 -100을 대체하여 디코딩할 수 없는 경우를 처리합니다.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # 간단한 후처리\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments \n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "\tf\"dylanmengzhou/kobart-trans-en-ko-v2\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    # fp16=True,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.40.0'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import transformers\n",
    "\n",
    "# transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-6624eda3-65eeec410f1b774f50db9990;81eb6416-5c32-446e-bf8e-a4216047712e)\n\nInvalid username or password.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Seq2SeqTrainer\n\u001b[1;32m----> 3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\transformers\\trainer_seq2seq.py:57\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     45\u001b[0m     model: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m, nn\u001b[38;5;241m.\u001b[39mModule] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m     preprocess_logits_for_metrics: Optional[Callable[[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     56\u001b[0m ):\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# Override self.model.generation_config if a GenerationConfig is specified in args.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# Priority: args.generation_config > model.generation_config > default GenerationConfig.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\transformers\\trainer.py:569\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhub_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub:\n\u001b[1;32m--> 569\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_hf_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m    571\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39moutput_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\transformers\\trainer.py:3896\u001b[0m, in \u001b[0;36mTrainer.init_hf_repo\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m   3893\u001b[0m     repo_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_model_id\n\u001b[0;32m   3895\u001b[0m token \u001b[38;5;241m=\u001b[39m token \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_token\n\u001b[1;32m-> 3896\u001b[0m repo_url \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub_private_repo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   3897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhub_model_id \u001b[38;5;241m=\u001b[39m repo_url\u001b[38;5;241m.\u001b[39mrepo_id\n\u001b[0;32m   3898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush_in_progress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\huggingface_hub\\hf_api.py:3360\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[1;34m(self, repo_id, token, private, repo_type, exist_ok, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[0;32m   3357\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   3359\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3360\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3361\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   3362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[0;32m   3363\u001b[0m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-6624eda3-65eeec410f1b774f50db9990;81eb6416-5c32-446e-bf8e-a4216047712e)\n\nInvalid username or password."
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
