{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BartConfig, BartTokenizer, BartModel\n",
    "\n",
    "# config = BartConfig.from_pretrained(\"facebook/bart-large\")\n",
    "# tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "# model = BartModel.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dylanmengzhou/kobart-trans-en-ko-v2\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"dylanmengzhou/kobart-trans-en-ko-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Your input_length: 31 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "c:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Your input_length: 91 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'ìš°ë¦¬ ì£¼ ì˜ˆìˆ˜ ê·¸ë¦¬ìŠ¤ë„ì˜ ì€í˜œê°€ ë„ˆí¬ì™€ í•¨ê»˜ í•˜ì‹­ì‹œìš”. ì•„ë©˜. ì£¼'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 90 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'ê·¸ëŸ¬ë¯€ë¡œ í•˜ë£¨ ì•„ì¹¨ì— ì—­ë³‘ì´ ëª°ë ¤ì™€ ì£½ê³  ì• í†µí•¨ê³¼ ê¸°ê·¼ì´ ì˜¤ê³  ë¶ˆë¡œ'}]\n",
      "[{'translation_text': 'ê·¸ë¦¬ê³  ê·¸ë…€ì™€ í•¨ê»˜ ì‚´ë©´ì„œ ì²˜ì ˆí•œ ì‹œë ¨ì„ ê²ªê³  ë§›ìˆê²Œ ì‚´ì•˜ë˜ ë•…ì˜'}]\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"translation\", model=\"circulus/kobart-trans-ko-en-v2\")\n",
    "# pipe(\"ì˜¤ëŠ˜ ì ì‹¬ìœ¼ë¡œ ìŠ¤í…Œì´í¬ë¥¼ ë¨¹ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"translation\", model=\"dylanmengzhou/kobart-trans-en-ko-v2\")\n",
    "print(pipe(\"The grace of our Lord Jesus Christ be with you all. Amen.\"))\n",
    "print(pipe(\"Therefore shall her plagues come in one day, death, and mourning, and famine; and she shall be utterly burned with fire: for strong is the Lord God who judgeth her.\"))\n",
    "print(pipe(\"And the kings of the earth, who have committed fornication and lived deliciously with her, shall bewail her, and lament for her, when they shall see the smoke of her burning,\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ í…Œì´ë¸”ì˜ ë°ì´í„°ìˆ˜ëŠ” ì´ 31102ê°œ ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pymysql\n",
    "\n",
    "# ë°ì´í„° ë² ì´ìŠ¤ ì—°ê²°í•˜ê¸°\n",
    "conn = pymysql.connect(host='1.251.203.204',\n",
    "                       user='root',\n",
    "                       password='kdt5',\n",
    "                       db='Team4',\n",
    "                       charset='utf8',\n",
    "                       port=33065)\n",
    "\n",
    "curs = conn.cursor()\n",
    "\n",
    "# ê²€ìƒ‰ ëª…ë ¹ì–´ ì‚¬ìš© \n",
    "sql = \"SELECT eng.text as eng_text, kor.text as kor_text FROM language_en eng join language_ko kor on eng.id = kor.id;\"\n",
    "curs.execute(sql)\n",
    "result = curs.fetchall()\n",
    "print(\"í˜„ì¬ í…Œì´ë¸”ì˜ ë°ì´í„°ìˆ˜ëŠ” ì´ {}ê°œ ì…ë‹ˆë‹¤.\".format(len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdp\\AppData\\Local\\Temp\\ipykernel_24900\\1190723113.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql, conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng_text</th>\n",
       "      <th>kor_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the beginning God created the heaven and th...</td>\n",
       "      <td>íƒœì´ˆì— í•˜ë‚˜ë‹˜ì´ ì²œì§€ë¥¼ ì°½ì¡°í•˜ì‹œë‹ˆë¼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And the earth was without form, and void; and ...</td>\n",
       "      <td>ë•…ì´ í˜¼ëˆí•˜ê³  ê³µí—ˆí•˜ë©° í‘ì•”ì´ ê¹ŠìŒ ìœ„ì— ìˆê³  í•˜ë‚˜ë‹˜ì˜ ì‹ ì€ ìˆ˜ë©´ì— ìš´í–‰í•˜ì‹œë‹ˆë¼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>í•˜ë‚˜ë‹˜ì´ ê°€ë¼ì‚¬ëŒ€ ë¹›ì´ ìˆìœ¼ë¼ í•˜ì‹œë§¤ ë¹›ì´ ìˆì—ˆê³ </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And God saw the light, that it was good: and G...</td>\n",
       "      <td>ê·¸ ë¹›ì´ í•˜ë‚˜ë‹˜ì˜ ë³´ì‹œê¸°ì— ì¢‹ì•˜ë”ë¼ í•˜ë‚˜ë‹˜ì´ ë¹›ê³¼ ì–´ë‘ì›€ì„ ë‚˜ëˆ„ì‚¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And God called the light Day, and the darkness...</td>\n",
       "      <td>ë¹›ì„ ë‚®ì´ë¼ ì¹­í•˜ì‹œê³  ì–´ë‘ì›€ì„ ë°¤ì´ë¼ ì¹­í•˜ì‹œë‹ˆë¼ ì €ë…ì´ ë˜ë©° ì•„ì¹¨ì´ ë˜ë‹ˆ ì´ëŠ” ì²«ì§¸...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            eng_text  \\\n",
       "0  In the beginning God created the heaven and th...   \n",
       "1  And the earth was without form, and void; and ...   \n",
       "2  And God said, Let there be light: and there wa...   \n",
       "3  And God saw the light, that it was good: and G...   \n",
       "4  And God called the light Day, and the darkness...   \n",
       "\n",
       "                                            kor_text  \n",
       "0                                íƒœì´ˆì— í•˜ë‚˜ë‹˜ì´ ì²œì§€ë¥¼ ì°½ì¡°í•˜ì‹œë‹ˆë¼  \n",
       "1       ë•…ì´ í˜¼ëˆí•˜ê³  ê³µí—ˆí•˜ë©° í‘ì•”ì´ ê¹ŠìŒ ìœ„ì— ìˆê³  í•˜ë‚˜ë‹˜ì˜ ì‹ ì€ ìˆ˜ë©´ì— ìš´í–‰í•˜ì‹œë‹ˆë¼  \n",
       "2                        í•˜ë‚˜ë‹˜ì´ ê°€ë¼ì‚¬ëŒ€ ë¹›ì´ ìˆìœ¼ë¼ í•˜ì‹œë§¤ ë¹›ì´ ìˆì—ˆê³   \n",
       "3               ê·¸ ë¹›ì´ í•˜ë‚˜ë‹˜ì˜ ë³´ì‹œê¸°ì— ì¢‹ì•˜ë”ë¼ í•˜ë‚˜ë‹˜ì´ ë¹›ê³¼ ì–´ë‘ì›€ì„ ë‚˜ëˆ„ì‚¬  \n",
       "4  ë¹›ì„ ë‚®ì´ë¼ ì¹­í•˜ì‹œê³  ì–´ë‘ì›€ì„ ë°¤ì´ë¼ ì¹­í•˜ì‹œë‹ˆë¼ ì €ë…ì´ ë˜ë©° ì•„ì¹¨ì´ ë˜ë‹ˆ ì´ëŠ” ì²«ì§¸...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_sql(sql, conn)\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ\n",
    "conn.close()\n",
    "\n",
    "# ì¿¼ë¦¬ -> dataframeê²°ê³¼ ì¶œë ¥\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_iterator(df):\n",
    "    for i, row in df.iterrows():\n",
    "        yield row[\"eng_text\"], row[\"kor_text\"] \n",
    "\n",
    "split_idx = int(len(df) * 0.8)  # ì „ì²´ ë°ì´í„° ì¤‘ 80%ë¥¼ í›ˆë ¨ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©\n",
    "train_df = df.iloc[:] # ê± ë‹¤ ì“°ì„¸ìš¤\n",
    "valid_df = df.iloc[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {\"id\":[],\n",
    "              \"translation\":[]}\n",
    "cnt = 0\n",
    "for value in train_df.values:\n",
    "    \n",
    "    train_dict[\"id\"].append(cnt)\n",
    "    \n",
    "    tempdict = {}\n",
    "    tempdict[\"eng\"] = value[0]\n",
    "    tempdict[\"kor\"] = value[1]\n",
    "    train_dict[\"translation\"].append(tempdict)\n",
    "    # print(value)\n",
    "    cnt += 1\n",
    "    # break\n",
    "\n",
    "train_df = pd.DataFrame(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict = {\"id\":[],\n",
    "              \"translation\":[]}\n",
    "cnt = 0\n",
    "for value in valid_df.values:\n",
    "    \n",
    "    val_dict[\"id\"].append(cnt)\n",
    "    \n",
    "    tempdict = {}\n",
    "    tempdict[\"eng\"] = value[0]\n",
    "    tempdict[\"kor\"] = value[1]\n",
    "    val_dict[\"translation\"].append(tempdict)\n",
    "    # print(value)\n",
    "    cnt += 1\n",
    "    # break\n",
    "\n",
    "valid_df = pd.DataFrame(val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 31102\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 6221\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "split_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"validation\": Dataset.from_pandas(valid_df)  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ ë™ì¼í•œ ë°ì´í„°í”„ë ˆì„ì„ ì‚¬ìš©í•˜ë¯€ë¡œ trainê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬\n",
    "})\n",
    "\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eng': 'And the earth was without form, and void; and darkness was upon the face of the deep. And the Spirit of God moved upon the face of the waters.',\n",
       " 'kor': 'ë•…ì´ í˜¼ëˆí•˜ê³  ê³µí—ˆí•˜ë©° í‘ì•”ì´ ê¹ŠìŒ ìœ„ì— ìˆê³  í•˜ë‚˜ë‹˜ì˜ ì‹ ì€ ìˆ˜ë©´ì— ìš´í–‰í•˜ì‹œë‹ˆë¼'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][1][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"dylanmengzhou/kobart-trans-en-ko-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [14603, 309, 299, 21235, 20041, 15562, 16160, 20676, 19542, 20676, 304, 16160, 17884, 315, 21579, 29709, 24536, 20290, 310, 18509, 258, 24536, 18785, 15562, 306, 309, 22873, 20676, 19542, 1700, 316, 311, 14889, 21235, 17254, 26605, 19524, 21235, 18785, 300, 29686, 245, 14603, 309, 299, 21235, 14470, 311, 21738, 16805, 19524, 15464, 20223, 19281, 310, 317, 17752, 1700, 316, 311, 14889, 21235, 17254, 26605, 19524, 21235, 20676, 16881, 27621, 245], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [15495, 12034, 14929, 9869, 14058, 14061, 13644, 14363, 16816, 11711, 12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032, 18466, 19808, 18940, 26418]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"eng\"]\n",
    "ko_sentence = split_datasets[\"train\"][1][\"translation\"][\"kor\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target=ko_sentence)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–ë•…', 'ì´', 'â–í˜¼', 'ëˆ', 'í•˜ê³ ', 'â–ê³µ', 'í—ˆ', 'í•˜ë©°', 'â–í‘', 'ì•”', 'ì´', 'â–ê¹Š', 'ìŒ', 'â–ìœ„ì—', 'â–ìˆê³ ', 'â–í•˜ë‚˜ë‹˜ì˜', 'â–ì‹ ', 'ì€', 'â–ìˆ˜', 'ë©´ì—', 'â–ìš´í–‰', 'í•˜ì‹œ', 'ë‹ˆë¼']\n",
      "['â–ë•…', 'ì´', 'â–í˜¼', 'ëˆ', 'í•˜ê³ ', 'â–ê³µ', 'í—ˆ', 'í•˜ë©°', 'â–í‘', 'ì•”', 'ì´', 'â–ê¹Š', 'ìŒ', 'â–ìœ„ì—', 'â–ìˆê³ ', 'â–í•˜ë‚˜ë‹˜ì˜', 'â–ì‹ ', 'ì€', 'â–ìˆ˜', 'ë©´ì—', 'â–ìš´í–‰', 'í•˜ì‹œ', 'ë‹ˆë¼']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(ko_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"])) \n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128 \n",
    "\n",
    "def preprocessing_function(examples):\n",
    "    inputs = [ex[\"eng\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"kor\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/31102 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31102/31102 [00:02<00:00, 11340.37 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6221/6221 [00:00<00:00, 11416.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = split_datasets.map(\n",
    "\tpreprocessing_function, \n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'decoder_input_ids'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15495, 12034, 14929,  9869, 14058, 14061, 13644, 14363, 16816, 11711,\n",
       "         12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032, 18466,\n",
       "         19808, 18940, 26418],\n",
       "        [14392, 16613, 22279, 11207,  9776, 27888, 17698, 10213, 20853, 10518,\n",
       "         27888, 22658,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 15495, 12034, 14929,  9869, 14058, 14061, 13644, 14363, 16816,\n",
       "         11711, 12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032,\n",
       "         18466, 19808, 18940],\n",
       "        [    1, 14392, 16613, 22279, 11207,  9776, 27888, 17698, 10213, 20853,\n",
       "         10518, 27888, 22658,     3,     3,     3,     3,     3,     3,     3,\n",
       "             3,     3,     3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15495, 12034, 14929, 9869, 14058, 14061, 13644, 14363, 16816, 11711, 12034, 15397, 12013, 16782, 15438, 21420, 14094, 12005, 14032, 18466, 19808, 18940, 26418]\n",
      "[14392, 16613, 22279, 11207, 9776, 27888, 17698, 10213, 20853, 10518, 27888, 22658]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 3):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdp\\AppData\\Local\\Temp\\ipykernel_24900\\200089639.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n",
      "c:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/sacrebleu/sacrebleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [0, 0, 0, 0],\n",
       " 'totals': [11, 10, 9, 8],\n",
       " 'precisions': [0.0, 0.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 11,\n",
       " 'ref_len': 4}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"In the beginning God created the heaven and the earth.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "    \"íƒœì´ˆì— í•˜ë‚˜ë‹˜ì´ ì²œì§€ë¥¼ ì°½ì¡°í•˜ì‹œë‹ˆë¼\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [0, 0, 0, 0],\n",
       " 'totals': [29, 28, 27, 26],\n",
       " 'precisions': [0.0, 0.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 29,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"And God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament: and it was so.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "    \"í•˜ë‚˜ë‹˜ì´ ê¶ì°½ì„ ë§Œë“œì‚¬ ê¶ì°½ ì•„ë˜ì˜ ë¬¼ê³¼ ê¶ì°½ ìœ„ì˜ ë¬¼ë¡œ ë‚˜ë‰˜ê²Œ í•˜ì‹œë§¤ ê·¸ëŒ€ë¡œ ë˜ë‹ˆë¼\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    # ëª¨ë¸ì´ ì˜ˆì¸¡ logits ì´ìƒì„ ë°˜í™˜í•˜ëŠ” ê²½ìš°\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # ë¼ë²¨ì˜ -100ì„ ëŒ€ì²´í•˜ì—¬ ë””ì½”ë”©í•  ìˆ˜ ì—†ëŠ” ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # ê°„ë‹¨í•œ í›„ì²˜ë¦¬\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments \n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "\tf\"dylanmengzhou/kobart-trans-en-ko-v2\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    # fp16=True,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.40.0'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import transformers\n",
    "\n",
    "# transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-6624eda3-65eeec410f1b774f50db9990;81eb6416-5c32-446e-bf8e-a4216047712e)\n\nInvalid username or password.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Seq2SeqTrainer\n\u001b[1;32m----> 3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\transformers\\trainer_seq2seq.py:57\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     45\u001b[0m     model: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m, nn\u001b[38;5;241m.\u001b[39mModule] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m     preprocess_logits_for_metrics: Optional[Callable[[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     56\u001b[0m ):\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# Override self.model.generation_config if a GenerationConfig is specified in args.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# Priority: args.generation_config > model.generation_config > default GenerationConfig.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\transformers\\trainer.py:569\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhub_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub:\n\u001b[1;32m--> 569\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_hf_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m    571\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39moutput_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\transformers\\trainer.py:3896\u001b[0m, in \u001b[0;36mTrainer.init_hf_repo\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m   3893\u001b[0m     repo_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_model_id\n\u001b[0;32m   3895\u001b[0m token \u001b[38;5;241m=\u001b[39m token \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhub_token\n\u001b[1;32m-> 3896\u001b[0m repo_url \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub_private_repo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   3897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhub_model_id \u001b[38;5;241m=\u001b[39m repo_url\u001b[38;5;241m.\u001b[39mrepo_id\n\u001b[0;32m   3898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush_in_progress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\huggingface_hub\\hf_api.py:3360\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[1;34m(self, repo_id, token, private, repo_type, exist_ok, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[0;32m   3357\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   3359\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3360\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3361\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   3362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[0;32m   3363\u001b[0m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kdp\\anaconda3\\envs\\TORCH_NLP38\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-6624eda3-65eeec410f1b774f50db9990;81eb6416-5c32-446e-bf8e-a4216047712e)\n\nInvalid username or password."
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
